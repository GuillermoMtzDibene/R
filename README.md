# R
I will upload some data visualisation or analyses I have done with R, as well as some modellings.

<a href = "https://guillermomtzdibene.github.io/R/StrokePrediction.html">Strokes </a> In this notebook, I worked throught the stroke data set from the Data Analysis challenged website Kaggle. I trained several models on a highly disbalanced data set (binary outcome with 19:1 ratio between classes) and evaluated their performances using AUROC. Some conclusion are drawn at the end.

<a href = "https://guillermomtzdibene.github.io/R/Gapminder_visualisation.html">Gapminder </a> In this visualisation I plot both the life expectancy as well as the child birth rate for several countries and the world, and see the trend.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter4_exercises.html">ISL, Chapter 4</a> In this notebook I worked through exercises 10 and 11 of chapter 4. This chapter is about classification, including linear and quadratic discriminant analyses, and KNN. In Exercise 11, using KNN with (k = 3) and two variables, we produced a model with 88% accuracy, vs a 50% that random guessing would give.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter5_exercises.html">ISL, Chapter 5</a> In this notebook I worked through exercises 5, 6 and 9 of chapter 5. Exercise 7 is practicing execution of loops, while exercise 8 is a simulated data set, both exercises seemed unnecessary to add. In exercise 9, we used the bootstrap method to find the standard error for the first two quartiles.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter6_exercises.html">ISL, Chapter 6</a> In this notebook I worked through exercises 8, 9, 10 and 11 of chapter 6. This chapter was all about model selection using different criterions or models, including Ridge, Lasso, Principal Components and Partial Least Square regressions.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter7_exercises.html">ISL, Chapter 7</a> In this notebook I worked through exercises 6, 7, 8 and 9 of chapter 7. This chapter was all about extending linear model to more flexible models and generalised additive models.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter8_exercises.html">ISL, Chapter 8</a> In this notebook I worked through exercises 6, 7, 8, 9, 10 and 11 of chapter 8. This chapter was all about decision trees, random forests and boosting. Some models here improve error rates from previous chapter from 66% to 9% error.

<a href = "https://guillermomtzdibene.github.io/R/ISL_Chapter9_exercises.html">ISL, Chapter 9</a> In this notebook I worked through exercises 7 and 8 (the only ones that dealt with real data). This was simple svm and tuning of parameters from the <p style = "font-family: 'Courier New', monospace"> e1071 </p> library.
